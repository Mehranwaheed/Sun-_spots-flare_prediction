# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MqqKklYCYsJogAp9m3qh31VFaNR0tZNl
"""

import pandas as pd

df=pd.read_csv("/content/drive/MyDrive/Datasets/time_difference_2.csv")
df.sample(5)

"""#time difference within 24 hours"""

df['time_difference_1']=pd.to_timedelta(df['time_difference_1'].str.split().str[-1])
filtered_df = df[df['time_difference_1'] <pd.Timedelta(hours=24)]
filtered_df.sample(5)

"""#Match Region Numbers"""

def check_number_match(d):
    reference_str = str(d["NOAA"])
    target_str = str(d["Region_number"])

    # Check if the numbers completely match
    if reference_str == target_str:
        return "matched"

    # Check if the first 5 digits match
    elif reference_str[1:] == target_str[:]:
        return "matched"
    elif reference_str[2:] == target_str[:]:
        return "matched"

    # If neither condition is met
    else:
        return "Not matched"
filtered_df["NOAA_check"]=filtered_df.apply(check_number_match,axis=1)

"""#Disregarding UNmatched Regions"""

filtered_df1=filtered_df[filtered_df["NOAA_check"]=="matched"]

"""#only M & X class flares"""

filtered_df1=filtered_df1[filtered_df1['classification'].isin(["M", "X"])]

"""checking if the association algorithm worked fine or not"""

filtered_df1[filtered_df1['Date']=="2003-05-02"]

filtered_df1[filtered_df1['Date']=="2003-11-02"]

"""converting McIntosh classes into the numerical form for model training"""

def striping(f):
  return f["McIntosh_class"].strip()
filtered_df1["McIntosh classes"]=filtered_df1.apply(striping, axis=1)
filtered_df1["McIntosh classes"]= filtered_df1["McIntosh classes"].replace('', pd.NA)

filtered_df1["McIntosh classes"].isna().sum()

filtered_df1.dropna(subset=["McIntosh classes"],inplace=True)

filtered_df1[['Class1', 'Class2',"Class3","class4","class5","class6"]]=filtered_df1['McIntosh classes'].str.split("",expand=True)

filtered_df1.sample(5)

filtered_df1.drop(columns=["Class1","class5","class6"],inplace=True)

class_1={"A":0.10,"H":0.15,"B":0.30,"C":0.45,"D":0.60,"E":0.75,"F":0.90}
def map_class_1(class_1_value):
    return class_1.get(class_1_value, pd.NA)

filtered_df1["Numeric_class1"]=filtered_df1['Class2'].apply(map_class_1)

class_2={"X":0,"R":0.10,"S":0.30,"A":0.50,"H":0.70,"K":0.90}
def map_class_2(class_2_value):
    return class_2.get(class_2_value, pd.NA)

filtered_df1["Numeric_class2"]=filtered_df1['Class3'].apply(map_class_2)

class_3={"X":0,"O":0.10,"C":0.90,"I":0.50}
def map_class_3(class_3_value):
    return class_3.get(class_3_value, pd.NA)

filtered_df1["Numeric_class3"]=filtered_df1['class4'].apply(map_class_3)

filtered_df1.sample(5)

"""calculating normalized fourth component for model"""

class_2={"X":1,"R":2,"S":3,"A":4,"H":5,"K":6}
def map_class_2(f):
  return class_2.get(f, pd.NA)
filtered_df1["calc"]=filtered_df1['Class3'].apply(map_class_2)

def p_component(f):
  return f["calc"]/6
filtered_df1["p_component"]=filtered_df1.apply(p_component,axis=1)

def normalize(f):
  return f["area"]/1000
filtered_df1["normalized_area"]=filtered_df1.apply(normalize,axis=1)

def calculation(f):
  x=abs((f["normalized_area"]-f["p_component"])*f["normalized_area"])
  return x
filtered_df1["fourth_comp"]=filtered_df1.apply(calculation,axis=1)

filtered_df1.sample(5)

"""normalizing sunspot numbers"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
column_to_scale = 'Number of sunspots'

# Extract the column as a 2D array (reshape if needed)
column_data = filtered_df1[column_to_scale].values.reshape(-1, 1)
scaled_column = scaler.fit_transform(column_data)

# Update the DataFrame with the scaled values
filtered_df1[column_to_scale] = abs(scaled_column)
filtered_df1.sample(5)

filtered_df1.columns

"""dropping unnecessary columns"""

filtered_df1.drop(columns=['Unnamed: 0','Time', 'Mount Wilson Class', 'Region_number',
                           'McIntosh_class','Date_time_s','start_time', 'end_time', 'peak_time',
                           'type', 'NOAA','Classification+type', 'Date_time_f', 'time_difference_1',
                           'time_difference', 'NOAA_check', 'McIntosh classes','Class2', 'Class3',
                           'class4','calc','p_component'
                           ],inplace=True)    #,'xray_flux'

filtered_df1.set_index("Date",inplace=True)
filtered_df1.sort_index(inplace=True)

filtered_df1.tail(5)

"""area normalization"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
column_to_scale = 'area'

# Extract the column as a 2D array (reshape if needed)
column_data = filtered_df1[column_to_scale].values.reshape(-1, 1)
scaled_column = scaler.fit_transform(column_data)

# Update the DataFrame with the scaled values
filtered_df1[column_to_scale] = abs(scaled_column)

"""length normalizations"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
column_to_scale = 'length'

# Extract the column as a 2D array (reshape if needed)
column_data = filtered_df1[column_to_scale].values.reshape(-1, 1)
scaled_column = scaler.fit_transform(column_data)

# Update the DataFrame with the scaled values
filtered_df1[column_to_scale] = abs(scaled_column)

"""converting M and X labels into O and 1"""

filtered_df1['classification'] = filtered_df1['classification'].map({'M': 0, 'X': 1})
filtered_df1.sample(5)

filtered_df1.shape

filtered_df1.info()

filtered_df1.isna().sum()

filtered_df1.dropna(inplace=True)

filtered_df1["fourth_comp"]=filtered_df1['fourth_comp'].astype(float)
filtered_df1['Numeric_class2']=filtered_df1['Numeric_class2'].astype(float)
filtered_df1['Numeric_class3']=filtered_df1['Numeric_class3'].astype(float)

filtered_df1.groupby("classification").mean("xray_flux")

x=filtered_df1[filtered_df1['xray_flux']<=0.01]
x["classification"].value_counts()

import numpy as np
import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"]=(6,6)
unique_elements, counts_elements= np.unique(filtered_df1['classification'],return_counts=True)
fig1,ax1=plt.subplots()
ax1.pie(counts_elements,labels=unique_elements,autopct="%1.1f%%",shadow=True,startangle=90,textprops={"fontsize":18})
plt.show()

"""Data is highly imbalanced only 7.2% of X class flares and 92.8% are M class flares

splitting the data into train and test for training and testing the model
"""

X = filtered_df1.drop(columns=['classification',"xray_flux"], axis=1)
y = filtered_df1['classification']
round(X.shape[0]*0.80,0)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)

#X_train=X.iloc[0:11699,:]
#X_test=X.iloc[11699:,:]
#y_train=y.iloc[0:11699]
#y_test=y.iloc[11699:]

"""splitted the data into train and test sets data from 1981 to 2001 is selected for training and from 2001 to 2017 for testing the model

#logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
log_class=LogisticRegression()
grid={"C":10.0**np.arange(-2,3),"penalty":["l1","l2"]}
cv=KFold(n_splits=5,random_state=None,shuffle=False)
clf=GridSearchCV(log_class,grid,cv=cv,n_jobs=-1,scoring="f1_micro")
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""#RandomForest classifier"""

from sklearn.ensemble import RandomForestClassifier
#class_weight=dict({"0.1":1,"0.9":1000})
classifier=RandomForestClassifier()
grid={"criterion":["gini","entropy","log_loss"],"max_depth":[1,10,20,100],"max_features":["sqrt","log","None"]}
cv=KFold(n_splits=5,random_state=None,shuffle=False)
classifier=GridSearchCV(classifier,grid,cv=cv,n_jobs=-1,scoring="f1_macro")

classifier.fit(X_train,y_train)

y_pred=classifier.predict(X_test)
#print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
#print(classification_report(y_test,y_pred))

from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
conf_matrix = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['M', 'X'], yticklabels=['M', 'X'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""Due to imbalanced data models are not predicting X class flares at all so need to balance it

#SMOTomek
data balancing Technique
"""

from imblearn.combine import SMOTETomek
st=SMOTETomek()
X_train_st,y_train_st=st.fit_resample(X_train,y_train)

"""after doing the SMOTomek running the model again"""

from sklearn.ensemble import RandomForestClassifier
#class_weight=dict({"0.1":1,"0.9":1000})
classifier=RandomForestClassifier()
grid={"criterion":["gini","entropy","log_loss"],"max_depth":[1,10,20,100],"max_features":["sqrt","log","None"]}
cv=KFold(n_splits=5,random_state=None,shuffle=False)
classifier=GridSearchCV(classifier,grid,cv=cv,n_jobs=-1,scoring="f1_macro")

classifier.fit(X_train_st,y_train_st)

y_pred=classifier.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""Random forest only predicted 21 X class flares after doing the data balancing

#SMOTENN
"""

from imblearn.combine import SMOTEENN
smote_enn = SMOTEENN(random_state=42,sampling_strategy=0.90)
X_smoteenn, y_smoteenn = smote_enn.fit_resample(X_train, y_train)

"""#Random Forest"""

from sklearn.ensemble import RandomForestClassifier
#class_weight=dict({"0.1":1,"0.9":1000})
classifier=RandomForestClassifier()
grid={"criterion":["gini","entropy","log_loss"],"max_depth":[1,10,20,100],"max_features":["sqrt","log","None"]}
cv=KFold(n_splits=5,random_state=None,shuffle=False)
classifier=GridSearchCV(classifier,grid,cv=cv,n_jobs=-1,scoring="f1_macro")

classifier.fit(X_smoteenn,y_smoteenn)

y_pred=classifier.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""#Random oversampler"""

from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=42)
X_ros, y_ros = ros.fit_resample(X_train, y_train)

from sklearn.ensemble import RandomForestClassifier
#class_weight=dict({"0.1":1,"0.9":1000})
classifier=RandomForestClassifier()
grid={"criterion":["gini","entropy","log_loss"],"max_depth":[1,10,20,100],"max_features":["sqrt","log","None"]}
cv=KFold(n_splits=5,random_state=None,shuffle=False)
classifier=GridSearchCV(classifier,grid,cv=cv,n_jobs=-1,scoring="f1_macro")

classifier.fit(X_ros,y_ros)

best=classifier.best_estimator_
best.fit(X_ros,y_ros)

y_pred=best.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
print(classification_report(y_test,y_pred))

y_pred=classifier.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""#SVM

##1.1 -kernel="rbf", random_state=42,C=1,gamma=10 these parameters with the data balanced by using SMOTomek technique
"""

from sklearn.svm import SVC
clf = SVC(kernel="rbf", random_state=42,C=1,gamma=10)
clf.fit(X_train_st,y_train_st)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0.1', '0.9'], yticklabels=['0.1', '0.9'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""##1.2 -kernel="rbf", random_state=42,C=1,gamma=10 these parameters with the data balanced by using SMOTENN technique




"""

from sklearn.svm import SVC
clf = SVC(kernel="rbf", random_state=42,C=1,gamma=10)
clf.fit(X_smoteenn,y_smoteenn)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0.1', '0.9'], yticklabels=['0.1', '0.9'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""##1.3 -kernel="rbf", random_state=42,C=1,gamma=10 these parameters with the data balanced by using RANDOM OVERSAMPLER technique




"""

from sklearn.svm import SVC
clf = SVC(kernel="rbf", random_state=42,C=1,gamma=10)
clf.fit(X_ros,y_ros)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0.1', '0.9'], yticklabels=['0.1', '0.9'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""##2.1 -kernel="rbf", random_state=42,C=0.1,gamma=10 these parameters with the data balanced by using SMOTomek technique"""

from sklearn.svm import SVC
clf = SVC(kernel="rbf", random_state=42,C=0.1,gamma=10)
clf.fit(X_train_st,y_train_st)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0.1', '0.9'], yticklabels=['0.1', '0.9'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""##2.2 -kernel="rbf", random_state=42,C=0.1,gamma=10 these parameters with the data balanced by using SMOTENN technique"""

from sklearn.svm import SVC
clf = SVC(kernel="rbf", random_state=42,C=0.1,gamma=10)
clf.fit(X_smoteenn,y_smoteenn)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0.1', '0.9'], yticklabels=['0.1', '0.9'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""##2.3 -kernel="rbf", random_state=42,C=0.1,gamma=10 these parameters with the data balanced by using RANDOMOVERSAMPLER technique"""

from sklearn.svm import SVC
clf = SVC(kernel="rbf", random_state=42,C=0.1,gamma=10)
clf.fit(X_ros,y_ros)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0.1', '0.9'], yticklabels=['0.1', '0.9'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""##3.1 -kernel="rbf", random_state=42,C=1,gamma=1 these parameters with the data balanced by using SMOTomek technique"""

from sklearn.svm import SVC
clf = SVC(kernel="rbf", random_state=42,C=1,gamma=1)
clf.fit(X_train_st,y_train_st)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0.1', '0.9'], yticklabels=['0.1', '0.9'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""##3.2 -kernel="rbf", random_state=42,C=1,gamma=1 these parameters with the data balanced by using SMOTENN technique"""

from sklearn.svm import SVC
clf = SVC(kernel="rbf", random_state=42,C=1,gamma=10)
clf.fit(X_smoteenn,y_smoteenn)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0.1', '0.9'], yticklabels=['0.1', '0.9'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""##3.3 -kernel="rbf", random_state=42,C=1,gamma=1 these parameters with the data balanced by using RANDOMOVERSAMPLER technique"""

from sklearn.svm import SVC
clf = SVC(kernel="rbf", random_state=42,C=1,gamma=10)
clf.fit(X_ros,y_ros)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0.1', '0.9'], yticklabels=['0.1', '0.9'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""#Trying cost sensitive technique"""

class_weights = {0: 1, 1: 10}
from sklearn.svm import SVC
clf = SVC(kernel="rbf", random_state=42,C=1,gamma=10,class_weight=class_weights)
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0', '1'], yticklabels=['0', '1'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
# Example: Fine-tune hyperparameters
xgb_model_tuned = XGBClassifier(random_state=42)

#xgb_model_tuned.fit(X_train_st, y_train_st)
#y_pred_tuned = xgb_model_tuned.predict(X_test)
#accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
#print("Tuned Model Accuracy:", accuracy_tuned)

n_estimators = [10,20,60,100,120]
learning_rate=[0.1,0.4,0.6,1,1.2]
max_depth = [2,8,None]
subsample = [0.5,0.75,1.0]
colsample_bytree=[0.5,0.75,1.0]

param_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth,
              'learning_rate':learning_rate,
              'subsample':subsample,
              'colsample_bytree':colsample_bytree
             }
print(param_grid)

from sklearn.model_selection import RandomizedSearchCV

rf_grid1 = RandomizedSearchCV(estimator = xgb_model_tuned,
                       param_distributions = param_grid,
                       cv = 5,
                       verbose=2,
                       n_jobs = -1)

rf_grid1.fit(X_train_st,y_train_st)

best_model1=rf_grid1.best_estimator_
y_pred = best_model1.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
#print(f'Best Parameters: {best_params}')
print(f'Accuracy: {accuracy:.2f}')

plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0', '1'], yticklabels=['0', '1'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""#ENSEMBLE METHOD"""

from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier


#class_weights = {0: 0.3, 1: 3}         #kernel="rbf", random_state=42,C=1,gamma=10
model1 = SVC(probability=True)
model2 = RandomForestClassifier(random_state=42)
model3 = XGBClassifier()

"""learning_rate=0.1,
    max_depth=3,
    n_estimators=100,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
"""

weights = [1, 1, 1]
ensemble_model = VotingClassifier(estimators=[('rf', model1), ('svm', model2), ('lr', model3)], voting='soft',weights=weights)

ensemble_model.fit(X_train_st, y_train_st)

cross_val_accuracy = cross_val_score(ensemble_model, X_train_st, y_train_st, cv=5, scoring='accuracy')
print("Cross-Validation Accuracy:", cross_val_accuracy.mean())

y_pred = ensemble_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Set Accuracy:", accuracy)

plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0', '1'], yticklabels=['0', '1'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""#Gradient boosting"""

from sklearn.ensemble import GradientBoostingClassifier

# Initialize the Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)

# Fit the model
model=gb_classifier.fit(X_train_st, y_train_st)

# Get feature importances
feature_importances = gb_classifier.feature_importances_

# Create a DataFrame to visualize feature importance
feature_importance_df = pd.DataFrame(
    {'Feature': X_train.columns, 'Importance': feature_importances}
)

# Sort features by importance
feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)

# Plot the feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importance')
plt.show()

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
#print(f'Best Parameters: {best_params}')
print(f'Accuracy: {accuracy:.2f}')

plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0', '1'], yticklabels=['0', '1'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""#PCA"""

import numpy as np
import pandas as pd
import sklearn
import matplotlib.pylab as plt
import seaborn as sns
from sklearn import preprocessing
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA, KernelPCA
from sklearn import decomposition #PCA

scaler=StandardScaler()
X_train_st=scaler.fit_transform(X_train_st)
X_test=scaler.transform(X_test)
pca=PCA(n_components=8)

X_train_trf=pca.fit_transform(X_train_st)
X_test_trf=pca.transform(X_test)

def plot_pca_variance(e):
    plt.rc('grid', linestyle="--", color='black')
    PCs = ['PC_{}'.format(i+1) for i in range(len(e))]
    plt.scatter(PCs, e*100)
    plt.grid()

print('varinace', pca.explained_variance_ratio_*100)
plot_pca_variance(pca.explained_variance_ratio_)

per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)
labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]

plt.bar(x=range(1,len(per_var)+1), height=per_var , tick_label=labels)
plt.ylabel('Percentage of Explained Variance')
plt.xlabel('Principal Component')
plt.title('Scree Plot')
plt.show()

"""three PCA components are explaining almost 80% variance so decided to use 3 components for model building"""

plt.plot(np.cumsum(pca.explained_variance_ratio_))

"""#PCA with 3 PCA components"""

from sklearn.decomposition import PCA



# Specify the number of components (in this case, 3)
n_components = 3

# Initialize PCA with the desired number of components
pca = PCA(n_components=n_components)

# Fit the PCA model and transform your data
X_pca = pca.fit_transform(X)

# X_pca now contains the transformed data with three principal components

# If you want to access the principal components themselves
principal_components = pca.components_

# The explained variance ratio for each component
explained_variance_ratio = pca.explained_variance_ratio_

# Print the cumulative explained variance
cumulative_explained_variance = np.cumsum(explained_variance_ratio)
print("Cumulative Explained Variance:", cumulative_explained_variance[-1])

# You can also visualize the cumulative explained variance if needed

"""#Model training with 3 components

"""

round(X_pca.shape[0]*0.80)

X_train=X_pca[0:11699]
X_test=X_pca[11699:]
y_train=y.iloc[0:11699]
y_test=y.iloc[11699:]

class_weights = {0: 0.3, 1: 3}
from sklearn.svm import SVC
clf = SVC(kernel="rbf", random_state=42,C=1,gamma=10,class_weight=class_weights)
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0', '1'], yticklabels=['0', '1'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

"""applying smote technique"""

from sklearn.model_selection import GridSearchCV
class_weights = {0: 0.3, 1: 3}
clf=GridSearchCV(estimator=SVC(random_state=42,class_weight=class_weights),
             param_grid={'C': [1, 5,10], 'kernel': ('linear', 'rbf',"poly","sigmoid"),'gamma':[1, 5,10]})
clf.fit(X_train,y_train)

best_model=clf.best_estimator_

best_model.fit(X_train,y_train)

y_pred=best_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy : {accuracy:.2f}')
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d",  xticklabels=['0', '1'], yticklabels=['0', '1'],cbar=False,cmap="cividis")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

