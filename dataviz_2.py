# -*- coding: utf-8 -*-
"""Dataviz_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DBM5-UCq1GTc_AxBjV24nDy6NV2bTJw3
"""

import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt

df_flare=pd.read_csv("/content/drive/MyDrive/Datasets/flares_dataset_v1.csv",low_memory=False)
df_sunspot=pd.read_csv("/content/drive/MyDrive/Datasets/sunspot_dataset_v1.csv",low_memory=False)

df_flare.sample(5)

df_sunspot.head()

df_flare.info()

df_flare["Date"].astype(str)

df_flare["Date"]=pd.to_datetime(df_flare["Date"],errors="coerce")
df_flare['start_time']=pd.to_datetime(df_flare['start_time'],errors='coerce')
df_flare['end_time']=pd.to_datetime(df_flare['end_time'],errors='coerce')
df_flare['peak_time']=pd.to_datetime(df_flare['peak_time'],errors='coerce')

df_flare.head()

df_flare.dropna(subset=["NOAA","start_time","end_time","peak_time","classification","type"],inplace=True)

def merging(f):
  x=pd.to_datetime(f["Date"]).date()
  y=pd.to_datetime(f["start_time"]).time()
  z=f"{x} {y}"
  return pd.to_datetime(z)

df_flare["Date_time_f"]=df_flare.apply(merging,axis=1)

df_flare.sample(5)

df_flare.info()

df_flare.isna().sum()

df_flare.drop(columns=["observartories","meridain_date","xray_flux_sci","year","location"],inplace=True)

df_flare.isna().sum()

df_flare["xray_flux"]=df_flare["xray_flux"].fillna(df_flare["xray_flux"].mean())

df_flare.isna().sum()

df_flare.shape

df_flare['NOAA']=df_flare['NOAA'].astype(int)
df_flare['type']=df_flare['type'].astype(int)

#df_flare['NOAA']=df_flare['NOAA'].astype(str)
#df_flare['type']=df_flare['type'].astype(str)

df_flare.info()

df_sunspot.info()

df_sunspot.drop(columns=["individual date","Location","regional date","station_number","observartories"],inplace=True)

df_sunspot.info()

df_sunspot['Date']=pd.to_datetime(df_sunspot['Date'],errors="coerce")
df_sunspot['Time']=pd.to_datetime(df_sunspot['Time'],errors="coerce")
df_sunspot['Region_number']=df_sunspot['Region_number']

df_sunspot.dropna(subset=["Time","Date"],inplace=True)

df_sunspot.sample(5)

def merging(f):
  x=pd.to_datetime(f["Date"]).date()
  y=pd.to_datetime(f["Time"]).time()
  z=f"{x} {y}"
  return pd.to_datetime(z)

df_sunspot["Date_time_s"]=df_sunspot.apply(merging,axis=1)

df_sunspot.sample(5)

df_sunspot.isna().sum()

df_sunspot.isna().sum()

df_sunspot['Number of sunspots']=df_sunspot['Number of sunspots'].fillna(df_sunspot['Number of sunspots'].mean())
df_sunspot['length']=df_sunspot['length'].fillna(df_sunspot['length'].mean())
df_sunspot['area']=df_sunspot['area'].fillna(df_sunspot['area'].mean())

df_sunspot.isna().sum()

df_flare.head()



df_flare.shape

df_flare[df_flare['Date']=="2007-01-04"]

#df_sunspot['Date']=pd.to_datetime(df_sunspot['Date']).dt.tz_localize(None)

df_sunspot.info()

df_sunspot.head()

#df_flare[df_flare['Date']>="1981-01-03"]

df_flare.head()

df_flare.info()

df_sunspot.head()

merged_df = pd.merge(df_sunspot, df_flare,on='Date',how='left')
merged_df.head()

merged_df.info()

merged_df.isna().sum()

merged_df.dropna(subset=["start_time"],inplace=True)

merged_df.sample(5)

merged_df.isna().sum()

merged_df.dropna(subset=["Region_number"],inplace=True)

merged_df.isna().sum()

merged_df.sample(5)

from datetime import datetime

def calculate_time_difference(g):
    try:
        # Convert datetime strings to datetime objects
        start_datetime = datetime.strptime(str(g["Time"]), "%Y-%m-%d %H:%M:%S")
        end_datetime = datetime.strptime(str(g["start_time"]), "%Y-%m-%d %H:%M:%S")

        # Calculate the time difference
        time_difference = end_datetime - start_datetime
        if time_difference.total_seconds() < 0:
          return start_datetime - end_datetime
        else:
          return time_difference

            #time_difference = start_datetime - end_datetime




    except ValueError as e:
        return f"Error: {e}"

def time(f):
  return f["Date_time_f"]-f["Date_time_s"]

merged_df["time_difference_1"]=merged_df.apply(time,axis=1)

merged_df["time_difference"]=merged_df.apply(calculate_time_difference,axis=1)

merged_df.sample(5)

merged_df['Time']=merged_df['Time'].dt.time
merged_df['start_time']=merged_df['start_time'].dt.time
merged_df['end_time']=merged_df['end_time'].dt.time
merged_df['peak_time']=merged_df['peak_time'].dt.time

merged_df.to_csv("/content/drive/MyDrive/Datasets/time_difference_2.csv")

merged_df.sample(5)

def check_number_match(d):
    reference_str = str(d["NOAA"])
    target_str = str(d["Region_number"])

    # Check if the numbers completely match
    if reference_str == target_str:
        return "matched"

    # Check if the first 5 digits match
    elif reference_str[1:] == target_str[:]:
        return "matched"
    elif reference_str[2:] == target_str[:]:
        return "matched"

    # If neither condition is met
    else:
        return "Not matched"

filtered_df["check"]=filtered_df.apply(check_number_match,axis=1)

filtered_df.sample(5)

df=filtered_df[filtered_df['check']=="matched"]

df_1=filtered_df[filtered_df['check']=="Not matched"]

df.sample(5)

df.shape

df_1['classification'].value_counts()

df['classification'].value_counts()

plt.figure(figsize=(28,6))
plt.plot(df["Date"],df['Number of sunspots'])
plt.ylabel("Number of Sunspots")
plt.xlabel("Date")
plt.title("Number of Sunspots")

plt.figure(figsize=(28,6))
plt.plot(df_1["Date"],df_1['Number of sunspots'])
plt.ylabel("Number of Sunspots")
plt.xlabel("Date")
plt.title("Un Matched Number of Sunspots")

plt.figure(figsize=(28,6))
plt.plot(df["Date"],df['xray_flux'])
plt.ylabel("Xray Flux")
plt.xlabel("Date")
plt.title("Xray Flux")
plt.yscale("log")

plt.figure(figsize=(28,6))
plt.plot(df_1["Date"],df_1['xray_flux'])
plt.ylabel("Xray Flux")
plt.xlabel("Date")
plt.title("Xray Flux")
plt.yscale("log")

import seaborn as sns

sns.heatmap(df.corr())

df[df["Date"]=="2003-11-02"]

"""to confirm that i associated the sunspot features correctly or not checked with the picture in presentation where he showed this is associated feature of the flare and it's matching perfectly"""

#df_final_v1['Time']=df_final_v1['Time'].dt.time
df['start_time']=df['start_time'].dt.time
#df_final_v1['end_time']=df_final_v1['end_time'].dt.time
#df_final_v1['peak_time']=df_final_v1['peak_time'].dt.time

df_1['start_time']=df_1['start_time'].dt.time

df_1_filtered = df_1[df_1["classification"].isin(["M", "X"])]
df_1_filtered.sample(5)

df_filtered = df[df["classification"].isin(["M", "X"])]
df_filtered.sample(5)

df_filtered.shape

df_1_filtered.shape

df_filtered["Time"]=df_filtered["Time"].dt.time

df_1_filtered["Time"]=df_1_filtered["Time"].dt.time

df_filtered.sample(5)

df_filtered["classification"].value_counts()

df_1_filtered["classification"].value_counts()

"""after association we have found out that M class flares that we associated are 4046 and X class flares are just 296"""

df_filtered["NOAA"].nunique()

df_1_filtered["NOAA"].nunique()

"""these flares were produced by 861 different sunspot groups"""

df_filtered["NOAA"].value_counts()

"""the groups whicha are major contributor to M and X class flares are

---
5395       105


3804       90


5312       57


5669       54


4964       49

---




---



"""

df_1_filtered["NOAA"].value_counts()

df_filtered["McIntosh_class"].value_counts()

df_1_filtered["McIntosh_class"].value_counts()

df_filtered.drop(columns=["Classification+type",'time_difference',"check"],inplace=True)

df_1_filtered.drop(columns=["Classification+type",'time_difference',"check"],inplace=True)

df_filtered.drop(columns=["end_time",'peak_time'],inplace=True)

df_1_filtered.drop(columns=["end_time",'peak_time'],inplace=True)

df_filtered.sample(5)

df_1_filtered.head()

df_filtered.tail()

df_1_filtered=df_1_filtered.set_index('Date')

df_filtered=df_filtered.set_index('Date')

df_filtered.head()

df_filtered.drop(columns=["Time","start_time"],inplace=True)

df_1_filtered.drop(columns=["Time","start_time"],inplace=True)

df_filtered.drop(columns=["Mount Wilson Class","Date_time_s","Date_time_f"],inplace=True)

df_1_filtered.drop(columns=["Mount Wilson Class","Date_time_s","Date_time_f"],inplace=True)

df_1_filtered.sample(5)

# Assuming df is your time series DataFrame
train_size = int(len(df_filtered) * 0.8)  # 80% for training
train_data, test_data = df_filtered[:train_size], df_filtered[train_size:]

X_train, y_train = train_data.drop('classification', axis=1), train_data['classification']
X_test, y_test = test_data.drop('classification', axis=1), test_data['classification']

X_train.shape

X_test.shape

y_train.shape

y_test.shape

from sklearn import svm

classifier=svm.SVC(kernel="rbf",gamma="scale",C=2)
classifier.fit(X_train,y_train)

y_pred=classifier.predict(X_test)

from sklearn.metrics import classification_report

print(classification_report(y_test,y_pred))

from sklearn.metrics import accuracy_score, confusion_matrix

conf_matrix = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['Non-flare', 'M-class', 'X-class'], yticklabels=['Non-flare', 'M-class', 'X-class'])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

df_1_filtered.head()

df_copy=df_filtered.copy()

df_copy=df_copy.set_index("Date")

df_copy.sample(5)

df_copy.drop(columns=["Time","Mount Wilson Class","Region_number","Date_time_s","start_time","end_time","peak_time","type","NOAA","xray_flux","Classification+type","Date_time_f","time_difference","check"],inplace=True)

df_copy.sample(5)

df_1_filtered.sample(5)

df_1_filtered.sample(5)

df_1_filtered.drop(columns=["Region_number","NOAA","xray_flux"],inplace=True)

df_1_filtered['McIntosh_class'].str.strip().eq('').sum()

df_copy['McIntosh_class'].str.strip().eq('').sum()

df_copy['McIntosh_class'].apply(lambda x: x.isspace()).sum()

df_copy['McIntosh_class'] = df_copy['McIntosh_class'].replace(r'^\s*$', pd.NA, regex=True)

df_1_filtered['McIntosh_class'] = df_1_filtered['McIntosh_class'].replace(r'^\s*$', pd.NA, regex=True)

df_copy.isna().sum()

df_1_filtered.isna().sum()

df_copy.info()

df_1_filtered.dropna(inplace=True)

df_copy.dropna(inplace=True)

df_copy['McIntosh_class'].value_counts()

df_1_filtered['McIntosh_class'].value_counts()

df_copy[['Class0', 'Class1', 'Class2',"Class3"]] = df_copy['McIntosh_class'].str.split('', expand=True).iloc[:, 1:5]

df_1_filtered[['Class0', 'Class1', 'Class2',"Class3"]] = df_1_filtered['McIntosh_class'].str.split('', expand=True).iloc[:, 1:5]

df_copy.sample(5)

df_1_filtered.sample(5)

df_copy.drop(columns=["Class0"],inplace=True)

df_1_filtered.drop(columns=["Class0"],inplace=True)

df_copy.sample(5)

class_1={"A":0.10,"H":0.15,"B":0.30,"C":0.45,"D":0.60,"E":0.75,"F":0.90}
def map_class_1(class_1_value):
    return class_1.get(class_1_value, pd.NA)
df_copy['numeric_class_1'] = df_copy['Class1'].apply(map_class_1)
df_1_filtered['numeric_class_1'] = df_1_filtered['Class1'].apply(map_class_1)

class_2={"X":0,"R":0.10,"S":0.30,"A":0.50,"H":0.70,"K":0.90}
def map_class_2(class_2_value):
    return class_2.get(class_2_value, pd.NA)
df_copy['numeric_class_2'] = df_copy['Class2'].apply(map_class_2)

df_1_filtered['numeric_class_2'] = df_1_filtered['Class2'].apply(map_class_2)

class_3={"X":0,"O":0.10,"C":0.90,"I":0.50}
def map_class_3(class_3_value):
    return class_3.get(class_3_value, pd.NA)
df_copy['numeric_class_3'] = df_copy['Class3'].apply(map_class_3)

df_1_filtered['numeric_class_3'] = df_1_filtered['Class3'].apply(map_class_3)

df_copy.sample(5)

df_1_filtered.sample(5)

df_copy.isna().sum()

df_1_filtered.isna().sum()

df_copy.dropna(inplace=True)

df_1_filtered.dropna(inplace=True)

df_copy.shape

df_1_filtered.shape

res={"M":0.1,"X":0.9}
def map_class_4(class_3_value):
    return res.get(class_3_value, pd.NA)
df_copy['target'] = df_copy['classification'].apply(map_class_4)

df_1_filtered['target'] = df_1_filtered['classification'].apply(map_class_4)

df_copy.sample(5)

df_1_filtered.sample(5)

df_copy.drop(columns=["McIntosh_class","classification","Class1","Class2","Class3"],inplace=True)

df_1_filtered.drop(columns=["McIntosh_class","classification","Class1","Class2","Class3","type"],inplace=True)

df_copy.sample(5)

df_1_filtered.sample(5)

df_copy.info()

df_copy['target']=df_copy['target'].astype(object)

df_1_filtered['target']=df_1_filtered['target'].astype(object)

df_copy['target'].value_counts()

df_1_filtered['target'].value_counts()

from sklearn.preprocessing import LabelEncoder

# Assuming df is your DataFrame and 'target' is your target variable
label_encoder = LabelEncoder()
df_1_filtered['target'] = label_encoder.fit_transform(df_1_filtered['target'])

from sklearn.preprocessing import LabelEncoder

# Assuming df is your DataFrame and 'target' is your target variable
label_encoder = LabelEncoder()
df_copy['target'] = label_encoder.fit_transform(df_copy['target'])

df_1_filtered.sample(5)

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Assuming your DataFrame is named df

# Define features (X) and target variable (y)
X = df_1_filtered[['Number of sunspots', 'length', 'area', 'numeric_class_1', 'numeric_class_2', 'numeric_class_3']]
y = df_1_filtered['target']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create an SVM classifier
clf = SVC(kernel='rbf', random_state=42,class_weight="balanced")

# Train the classifier on the training set
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

print(classification_report(y_test,y_pred))

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Assuming your DataFrame is named df

# Define features (X) and target variable (y)
X = df_copy[['Number of sunspots', 'length', 'area', 'numeric_class_1', 'numeric_class_2', 'numeric_class_3']]
y = df_copy['target']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create an SVM classifier
clf = SVC(kernel='linear', random_state=42,class_weight="balanced",C=2)

# Train the classifier on the training set
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

conf_matrix = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['Non-flare', 'M-class', 'X-class'], yticklabels=['Non-flare', 'M-class', 'X-class'])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

import numpy as np
np.bincount(y_train)

from imblearn.over_sampling import SMOTE

sm=SMOTE(random_state=42)
X_res,y_res=sm.fit_resample(X_train,y_train)

np.bincount(y_res)

# Create an SVM classifier
clf = SVC(kernel='linear', random_state=42,class_weight="balanced")

# Train the classifier on the training set
clf.fit(X_res, y_res)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

