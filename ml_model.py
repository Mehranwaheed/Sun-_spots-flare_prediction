# -*- coding: utf-8 -*-
"""ML_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bilK8GCo9-Duxts2a-Dxe6hCSx0AZtb6
"""

import numpy as np
import pandas as pd
import seaborn as sns
import plotly.express as px
from matplotlib import pyplot as plt
from sklearn.model_selection import cross_val_score

from sklearn import metrics
from collections import Counter

import pandas as pd
df=pd.read_csv("/content/drive/MyDrive/Datasets/model_data.csv")
df.head()

df.info()

def changing(f):
  if f["flare_type"]==0.1:
    return 0
  else:
    return 1
df["flare_type"]=df.apply(changing,axis=1)

#df['flaring_possiblity']=df['flaring_possiblity'].astype(str)
#df['flare_type']=df['flare_type'].astype(str)

df.set_index("Date",inplace=True)
df.sort_index(inplace=True)

x=round(df.shape[0]*0.80)

"""splitting into train test"""

X=df.drop(columns=["flaring_possiblity","flare_type"],axis=1)
y=df["flare_type"]

X_train=X.iloc[:x,:]
X_test=X.iloc[x:,:]
y_train=y.iloc[:x]
y_test=y.iloc[x:]

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"]=(6,6)
unique_elements, counts_elements= np.unique(y,return_counts=True)
fig1,ax1=plt.subplots()
ax1.pie(counts_elements,labels=unique_elements,autopct="%1.1f%%",shadow=True,startangle=90,textprops={"fontsize":18})
plt.show()

plt.figure(figsize=(3,4))
ax = sns.countplot(x='flare_type',data=df,palette="pastel")
for i in ax.containers:
    ax.bar_label(i,)

X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)

# Define a dataframe containing frequency percentages
df_perc = pd.concat([y.value_counts(normalize=True).mul(100).round(1),
                     y_train_new.value_counts(normalize=True).mul(100).round(1),
                     y_test_new.value_counts(normalize=True).mul(100).round(1)], axis=1)
df_perc.columns=['Dataset','Training','Test']
df_perc = df_perc.T

# Plot frequency percentages barplot
df_perc.plot(kind='barh', stacked=True, figsize=(10,5), width=0.6)

# Add the percentages to our plot
for idx, val in enumerate([*df_perc.index.values]):
    for (percentage, y_location) in zip(df_perc.loc[val], df_perc.loc[val].cumsum()):
        plt.text(x=(y_location - percentage) + (percentage / 2)-3,
                 y=idx - 0.05,
                 s=f'{percentage}%',
                 color="black",
                 fontsize=12,
                 fontweight="bold")



"""#Testing"""

from sklearn.metrics import f1_score,accuracy_score
from xgboost import XGBClassifier
from imblearn.ensemble import BalancedRandomForestClassifier
from imblearn.pipeline import Pipeline, make_pipeline
from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
from imblearn.ensemble import EasyEnsembleClassifier

from sklearn.ensemble import BaggingClassifier

from imblearn.ensemble import BalancedBaggingClassifier

bagging = BaggingClassifier(n_estimators=50, random_state=0)
balanced_bagging = BalancedBaggingClassifier(n_estimators=50, random_state=0)

bagging.fit(X_train_new, y_train_new)
balanced_bagging.fit(X_train_new, y_train_new)

y_pred_bc = bagging.predict(X_test_new)
y_pred_bbc = balanced_bagging.predict(X_test_new)

from sklearn.metrics import balanced_accuracy_score

from imblearn.metrics import geometric_mean_score
print("Bagging classifier performance:")
print(
    f"Balanced accuracy: {balanced_accuracy_score(y_test_new, y_pred_bc):.2f} - "
    f"Geometric mean {geometric_mean_score(y_test, y_pred_bc):.2f}"
)
print("Balanced Bagging classifier performance:")
print(
    f"Balanced accuracy: {balanced_accuracy_score(y_test_new, y_pred_bbc):.2f} - "
    f"Geometric mean {geometric_mean_score(y_test, y_pred_bbc):.2f}"
)

import matplotlib.pyplot as plt

fig, axs = plt.subplots(ncols=2, figsize=(10, 5))
ConfusionMatrixDisplay.from_estimator(
    bagging, X_test_new, y_test_new, ax=axs[0], colorbar=False
)
axs[0].set_title("Bagging")

ConfusionMatrixDisplay.from_estimator(
    balanced_bagging, X_test_new, y_test_new, ax=axs[1], colorbar=False
)
axs[1].set_title("Balanced Bagging")

fig.tight_layout()

from sklearn.ensemble import RandomForestClassifier

from imblearn.ensemble import BalancedRandomForestClassifier

rf = RandomForestClassifier(n_estimators=50, random_state=0)
brf = BalancedRandomForestClassifier(
    n_estimators=50, sampling_strategy="all", replacement=True, random_state=0
)

rf.fit(X_train_new, y_train_new)
brf.fit(X_train_new, y_train_new)

y_pred_rf = rf.predict(X_test_new)
y_pred_brf = brf.predict(X_test_new)

print("Random Forest classifier performance:")
print(
    f"Balanced accuracy: {balanced_accuracy_score(y_test_new, y_pred_rf):.2f} - "
    f"Geometric mean {geometric_mean_score(y_test_new, y_pred_rf):.2f}"
)
print("Balanced Random Forest classifier performance:")
print(
    f"Balanced accuracy: {balanced_accuracy_score(y_test_new, y_pred_brf):.2f} - "
    f"Geometric mean {geometric_mean_score(y_test_new, y_pred_brf):.2f}"
)

fig, axs = plt.subplots(ncols=2, figsize=(10, 5))
ConfusionMatrixDisplay.from_estimator(rf, X_test_new, y_test_new, ax=axs[0], colorbar=False)
axs[0].set_title("Random forest")

ConfusionMatrixDisplay.from_estimator(brf, X_test_new, y_test_new, ax=axs[1], colorbar=False)
axs[1].set_title("Balanced random forest")

fig.tight_layout()

from sklearn.ensemble import AdaBoostClassifier

from imblearn.ensemble import EasyEnsembleClassifier, RUSBoostClassifier

estimator = AdaBoostClassifier(n_estimators=10)
eec = EasyEnsembleClassifier(n_estimators=10, estimator=estimator)
eec.fit(X_train_new, y_train_new)
y_pred_eec = eec.predict(X_test_new)

rusboost = RUSBoostClassifier(n_estimators=10, estimator=estimator)
rusboost.fit(X_train_new, y_train_new)
y_pred_rusboost = rusboost.predict(X_test_new)

print("Easy ensemble classifier performance:")
print(
    f"Balanced accuracy: {balanced_accuracy_score(y_test_new, y_pred_eec):.2f} - "
    f"Geometric mean {geometric_mean_score(y_test_new, y_pred_eec):.2f}"
)
print("RUSBoost classifier performance:")
print(
    f"Balanced accuracy: {balanced_accuracy_score(y_test_new, y_pred_rusboost):.2f} - "
    f"Geometric mean {geometric_mean_score(y_test_new, y_pred_rusboost):.2f}"
)

fig, axs = plt.subplots(ncols=2, figsize=(10, 5))

ConfusionMatrixDisplay.from_estimator(eec, X_test_new, y_test_new, ax=axs[0], colorbar=False)
axs[0].set_title("Easy Ensemble")
ConfusionMatrixDisplay.from_estimator(
    rusboost, X_test_new, y_test_new, ax=axs[1], colorbar=False
)
axs[1].set_title("RUSBoost classifier")

fig.tight_layout()
plt.show()

ros=RandomOverSampler()
sm=SMOTE()

estimator = AdaBoostClassifier(n_estimators=10)
eec = EasyEnsembleClassifier(n_estimators=10, estimator=estimator)
model=make_pipeline(sm,ros,estimator)
model.fit(X_train_new,y_train_new)

from sklearn.metrics import classification_report
y_pred=model.predict(X_test_new)
print(classification_report(y_test_new, y_pred))

"""#RANDOMFOREST"""

from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier


kf = StratifiedKFold(n_splits=5, shuffle=False)

rf = RandomForestClassifier(n_estimators=100, random_state=13)

score = cross_val_score(rf, X_train_new, y_train_new, cv=kf, scoring='recall')
print("Cross Validation Recall scores are: {}".format(score))
print("Average Cross Validation Recall score: {}".format(score.mean()))

from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [4, 6, 10, 12],
    'random_state': [13]
}

grid_rf = GridSearchCV(rf, param_grid=params, cv=kf,
                          scoring='recall').fit(X_train_new, y_train_new)

print('Best parameters:', grid_rf.best_params_)
print('Best score:', grid_rf.best_score_)

y_pred = grid_rf.predict(X_test_new)
from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score
cm = confusion_matrix(y_test_new, y_pred)

rf_Recall = recall_score(y_test_new, y_pred)
rf_Precision = precision_score(y_test_new, y_pred)
rf_f1 = f1_score(y_test_new, y_pred)
rf_accuracy = accuracy_score(y_test_new, y_pred)

print(cm)

ndf = [(rf_Recall, rf_Precision, rf_f1, rf_accuracy)]

rf_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])
rf_score.insert(0, 'Random Forest with', 'No Under/Oversampling')
rf_score

"""#Random Oversampling"""

from imblearn.over_sampling import RandomOverSampler
# define oversampling strategy
ros = RandomOverSampler(random_state=42)

X_over, y_over = ros.fit_resample(X_train_new, y_train_new)

print('M:', y_over.value_counts()[0], '/', round(y_over.value_counts()[0]/len(y_over) * 100,2), '% of the dataset')
print('X:', y_over.value_counts()[1], '/',round(y_over.value_counts()[1]/len(y_over) * 100,2), '% of the dataset')

from imblearn.pipeline import Pipeline, make_pipeline

random_overs_pipeline = make_pipeline(RandomOverSampler(random_state=42),
                              RandomForestClassifier(n_estimators=100, random_state=13))

score2 = cross_val_score(random_overs_pipeline, X_train_new, y_train_new, scoring='recall', cv=kf)
print("Cross Validation Recall Scores are: {}".format(score2))
print("Average Cross Validation Recall score: {}".format(score2.mean()))

new_params = {'randomforestclassifier__' + key: params[key] for key in params}
grid_over_rf = GridSearchCV(random_overs_pipeline, param_grid=new_params, cv=kf, scoring='recall',
                        return_train_score=True)
grid_over_rf.fit(X_train_new, y_train_new)

print('Best parameters:', grid_over_rf.best_params_)
print('Best score:', grid_over_rf.best_score_)

y_pred = grid_over_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test_new)

cm = confusion_matrix(y_test_new, y_pred)

over_rf_Recall = recall_score(y_test_new, y_pred)
over_rf_Precision = precision_score(y_test_new, y_pred)
over_rf_f1 = f1_score(y_test_new, y_pred)
over_rf_accuracy = accuracy_score(y_test_new, y_pred)

print(cm)

ndf = [(over_rf_Recall, over_rf_Precision, over_rf_f1, over_rf_accuracy)]

over_rf_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])
over_rf_score.insert(0, 'Random Forest with', 'Random Oversampling')
over_rf_score

"""#Random UnderSampling"""

from imblearn.under_sampling import RandomUnderSampler
# define oversampling strategy
rus = RandomUnderSampler(random_state=42)

X_under, y_under = rus.fit_resample(X_train, y_train)

print('M:', y_under.value_counts()[0], '/', round(y_under.value_counts()[0]/len(y_under) * 100,2), '% of the dataset')
print('X:', y_under.value_counts()[1], '/',round(y_under.value_counts()[1]/len(y_under) * 100,2), '% of the dataset')

"""#SMOTE"""

from imblearn.over_sampling import SMOTE

smote_pipeline = make_pipeline(SMOTE(random_state=42),
                              RandomForestClassifier(n_estimators=100, random_state=13))

score3 = cross_val_score(smote_pipeline, X_train_new, y_train_new, scoring='recall', cv=kf)
print("Cross Validation Recall Scores are: {}".format(score3))
print("Average Cross Validation Recall score: {}".format(score3.mean()))

new_params = {'randomforestclassifier__' + key: params[key] for key in params}
smote_rf = GridSearchCV(smote_pipeline, param_grid=new_params, cv=kf, scoring='recall',
                        return_train_score=True)
smote_rf.fit(X_train_new, y_train_new)

print('Best parameters:', smote_rf.best_params_)
print('Best score:', smote_rf.best_score_)

y_pred = smote_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test_new)

cm = confusion_matrix(y_test_new, y_pred)

smote_rf_Recall = recall_score(y_test_new, y_pred)
smote_rf_Precision = precision_score(y_test_new, y_pred)
smote_rf_f1 = f1_score(y_test_new, y_pred)
smote_rf_accuracy = accuracy_score(y_test_new, y_pred)

print(cm)

ndf = [(smote_rf_Recall, smote_rf_Precision, smote_rf_f1, smote_rf_accuracy)]

smote_rf_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])
smote_rf_score.insert(0, 'Random Forest with', 'SMOTE Oversampling')
smote_rf_score

"""#Tomek Link"""

from imblearn.under_sampling import TomekLinks

# define the undersampling method
#tomekU = TomekLinks(sampling_strategy='auto', n_jobs=-1)
tomekU = TomekLinks()

# fit and apply the transform
X_underT, y_underT = tomekU.fit_resample(X_train_new, y_train_new)

print('M:', y_underT.value_counts()[0], '/', round(y_underT.value_counts()[0]/len(y_underT) * 100,2), '% of the dataset')
print('X:', y_underT.value_counts()[1], '/',round(y_underT.value_counts()[1]/len(y_underT) * 100,2), '% of the dataset')

"""Combining Tomek and SMOTE"""

from imblearn.combine import SMOTETomek

SMOTETomek_pipeline = make_pipeline(SMOTETomek(tomek=TomekLinks(sampling_strategy='majority')),
                              RandomForestClassifier(n_estimators=100, random_state=13))

SMOTETomek_rf = SMOTETomek_pipeline
SMOTETomek_rf.fit(X_train_new, y_train_new)

y_pred = SMOTETomek_rf.predict(X_test_new)

cm = confusion_matrix(y_test_new, y_pred)

SMOTETomek_rf_Recall = recall_score(y_test_new, y_pred)
SMOTETomek_rf_Precision = precision_score(y_test_new, y_pred)
SMOTETomek_rf_f1 = f1_score(y_test_new, y_pred)
SMOTETomek_rf_accuracy = accuracy_score(y_test_new, y_pred)
print(cm)

ndf = [(SMOTETomek_rf_Recall, SMOTETomek_rf_Precision, SMOTETomek_rf_f1, SMOTETomek_rf_accuracy)]

SMOTETomek_rf_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])
SMOTETomek_rf_score.insert(0, 'Random Forest with', 'SMOTE + Tomek')
SMOTETomek_rf_score

rfb = RandomForestClassifier(n_estimators=100, random_state=13, class_weight="balanced")

score5 = cross_val_score(rfb, X_train_new, y_train_new, cv=kf, scoring='recall')
print("Cross Validation Recall scores are: {}".format(score5))
print("Average Cross Validation Recall score: {}".format(score5.mean()))

grid_rfb = GridSearchCV(rfb, param_grid=params, cv=kf,
                          scoring='recall').fit(X_train_new, y_train_new)

y_pred = grid_rfb.predict(X_test_new)
cm = confusion_matrix(y_test_new, y_pred)

grid_rfb_Recall = recall_score(y_test_new, y_pred)
grid_rfb_Precision = precision_score(y_test_new, y_pred)
grid_rfb_f1 = f1_score(y_test_new, y_pred)
grid_rfb_accuracy = accuracy_score(y_test_new, y_pred)

print(cm)

ndf = [(grid_rfb_Recall, grid_rfb_Precision, grid_rfb_f1, grid_rfb_accuracy)]

grid_rfb_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])
grid_rfb_score.insert(0, 'Random Forest with', 'Class weights')
grid_rfb_score

predictions = pd.concat([rf_score, over_rf_score, smote_rf_score, SMOTETomek_rf_score, grid_rfb_score], ignore_index=True, sort=False)
predictions.sort_values(by=['Recall'], ascending=False)

from sklearn.metrics import roc_auc_score
ROCAUCscore = roc_auc_score(y_test_new, y_pred)
print(f"AUC-ROC Curve for Random Forest with Class weights: {ROCAUCscore:.4f}")

y_proba = grid_rfb.predict_proba(X_test_new)

from sklearn.metrics import roc_curve
from sklearn.metrics import RocCurveDisplay
def plot_auc_roc_curve(y_test_new, y_pred):
    fpr, tpr, _ = roc_curve(y_test_new, y_pred)
    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()
    roc_display.figure_.set_size_inches(5,5)
    plt.plot([0, 1], [0, 1], color = 'g')
# Plots the ROC curve using the sklearn methods - Good plot
plot_auc_roc_curve(y_test, y_proba[:, 1])
# Plots the ROC curve using the sklearn methods - Bad plot
#plot_sklearn_roc_curve(y_test, y_pred)

from sklearn.metrics import precision_recall_curve
from sklearn.metrics import PrecisionRecallDisplay

display = PrecisionRecallDisplay.from_estimator(
    grid_rfb, X_test_new, y_test_new, name="Average precision")
_ = display.ax_.set_title("Random Forest with Class weights")

"""#XGBOOST"""

from sklearn.model_selection import StratifiedKFold
from xgboost import XGBClassifier


kf = StratifiedKFold(n_splits=5, shuffle=False)

model = XGBClassifier(random_state=42)

score = cross_val_score(model, X_train_new, y_train_new, cv=kf, scoring='recall')
print("Cross Validation Recall scores are: {}".format(score))
print("Average Cross Validation Recall score: {}".format(score.mean()))

from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [4, 6, 10, 12],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'random_state': [13]
}


grid_rf = GridSearchCV(model, param_grid=params, cv=kf,
                          scoring='recall').fit(X_train_new, y_train_new)

print('Best parameters:', grid_rf.best_params_)
print('Best score:', grid_rf.best_score_)
y_pred = grid_rf.predict(X_test_new)
from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score
cm = confusion_matrix(y_test_new, y_pred)

rf_Recall = recall_score(y_test_new, y_pred)
rf_Precision = precision_score(y_test_new, y_pred)
rf_f1 = f1_score(y_test_new, y_pred)
rf_accuracy = accuracy_score(y_test_new, y_pred)

print(cm)

ndf_x = [(over_rf_Recall, over_rf_Precision, over_rf_f1, over_rf_accuracy)]

over_rf_score_x = pd.DataFrame(data = ndf_x, columns=['Recall','Precision','F1 Score', 'Accuracy'])
over_rf_score_x.insert(0, 'XGB Classifier with', 'Normal Data')
over_rf_score_x

"""#Oversampling"""

from imblearn.over_sampling import RandomOverSampler
# define oversampling strategy
ros = RandomOverSampler(random_state=42)

from imblearn.pipeline import Pipeline, make_pipeline

random_overs_pipeline = make_pipeline(RandomOverSampler(random_state=42),
                              XGBClassifier(random_state=42))

score2 = cross_val_score(random_overs_pipeline, X_train_new, y_train_new, scoring='recall', cv=kf)
print("Cross Validation Recall Scores are: {}".format(score2))
print("Average Cross Validation Recall score: {}".format(score2.mean()))

new_params = {'xgbclassifier__' + key: params[key] for key in params}
grid_over_rf = GridSearchCV(random_overs_pipeline, param_grid=new_params, cv=kf, scoring='recall',
                        return_train_score=True)
grid_over_rf.fit(X_train_new, y_train_new)

print('Best parameters:', grid_over_rf.best_params_)
print('Best score:', grid_over_rf.best_score_)

y_pred = grid_over_rf.best_estimator_.named_steps['xgbclassifier'].predict(X_test_new)

cm = confusion_matrix(y_test_new, y_pred)

over_rf_Recall = recall_score(y_test_new, y_pred)
over_rf_Precision = precision_score(y_test_new, y_pred)
over_rf_f1 = f1_score(y_test_new, y_pred)
over_rf_accuracy = accuracy_score(y_test_new, y_pred)

print(cm)

ndf_x = [(over_rf_Recall, over_rf_Precision, over_rf_f1, over_rf_accuracy)]

over_rf_score_x = pd.DataFrame(data = ndf_x, columns=['Recall','Precision','F1 Score', 'Accuracy'])
over_rf_score_x.insert(0, 'XGB Classifier with', 'Random Oversampling')
over_rf_score_x

"""#SMOTE"""

from imblearn.over_sampling import SMOTE

smote_pipeline = make_pipeline(SMOTE(random_state=42),
                              XGBClassifier(random_state=42,n_estimators=100))

score3 = cross_val_score(smote_pipeline, X_train_new, y_train_new, scoring='recall', cv=kf)
print("Cross Validation Recall Scores are: {}".format(score3))
print("Average Cross Validation Recall score: {}".format(score3.mean()))

new_params = {'xgbclassifier__' + key: params[key] for key in params}
smote_rf = GridSearchCV(smote_pipeline, param_grid=new_params, cv=kf, scoring='recall',
                        return_train_score=True)
smote_rf.fit(X_train_new, y_train_new)

print('Best parameters:', smote_rf.best_params_)
print('Best score:', smote_rf.best_score_)

y_pred = smote_rf.best_estimator_.named_steps['xgbclassifier'].predict(X_test_new)

cm = confusion_matrix(y_test_new, y_pred)

smote_rf_Recall = recall_score(y_test_new, y_pred)
smote_rf_Precision = precision_score(y_test_new, y_pred)
smote_rf_f1 = f1_score(y_test_new, y_pred)
smote_rf_accuracy = accuracy_score(y_test_new, y_pred)

print(cm)

ndf_x = [(smote_rf_Recall, smote_rf_Precision, smote_rf_f1, smote_rf_accuracy)]

smote_rf_score_x = pd.DataFrame(data = ndf_x, columns=['Recall','Precision','F1 Score', 'Accuracy'])
smote_rf_score_x.insert(0, 'XGB Classifier with', 'SMOTE Oversampling')
smote_rf_score_x

"""#TOMEK LINK"""

from imblearn.under_sampling import TomekLinks

# define the undersampling method
#tomekU = TomekLinks(sampling_strategy='auto', n_jobs=-1)
tomekU = TomekLinks()

# fit and apply the transform
X_underT, y_underT = tomekU.fit_resample(X_train_new, y_train_new)

from imblearn.combine import SMOTETomek

SMOTETomek_pipeline = make_pipeline(SMOTETomek(tomek=TomekLinks(sampling_strategy='majority')),
                              XGBClassifier(random_state=42,n_estimators=100))

SMOTETomek_rf = SMOTETomek_pipeline
SMOTETomek_rf.fit(X_train_new, y_train_new)

y_pred = SMOTETomek_rf.predict(X_test_new)

cm = confusion_matrix(y_test_new, y_pred)

SMOTETomek_rf_Recall = recall_score(y_test_new, y_pred)
SMOTETomek_rf_Precision = precision_score(y_test_new, y_pred)
SMOTETomek_rf_f1 = f1_score(y_test_new, y_pred)
SMOTETomek_rf_accuracy = accuracy_score(y_test_new, y_pred)
print(cm)

ndf_x = [(SMOTETomek_rf_Recall, SMOTETomek_rf_Precision, SMOTETomek_rf_f1, SMOTETomek_rf_accuracy)]

SMOTETomek_rf_score_x = pd.DataFrame(data = ndf_x, columns=['Recall','Precision','F1 Score', 'Accuracy'])
SMOTETomek_rf_score_x.insert(0, 'XGB Classifier with', 'SMOTE + Tomek')
SMOTETomek_rf_score_x

"""COST SENSITIVE LEARNING"""

rfb = XGBClassifier(n_estimators=100, random_state=13)

score5 = cross_val_score(rfb, X_train_new, y_train_new, cv=kf, scoring='recall')
print("Cross Validation Recall scores are: {}".format(score5))
print("Average Cross Validation Recall score: {}".format(score5.mean()))

grid_rfb = GridSearchCV(rfb, param_grid=params, cv=kf,
                          scoring='recall').fit(X_train_new, y_train_new)

y_pred = grid_rfb.predict(X_test_new)
cm = confusion_matrix(y_test_new, y_pred)

grid_rfb_Recall = recall_score(y_test_new, y_pred)
grid_rfb_Precision = precision_score(y_test_new, y_pred)
grid_rfb_f1 = f1_score(y_test_new, y_pred)
grid_rfb_accuracy = accuracy_score(y_test_new, y_pred)

print(cm)

ndf_x = [(grid_rfb_Recall, grid_rfb_Precision, grid_rfb_f1, grid_rfb_accuracy)]

grid_rfb_score_x = pd.DataFrame(data = ndf_x, columns=['Recall','Precision','F1 Score', 'Accuracy'])
grid_rfb_score_x.insert(0, 'XGB Classifier with', 'Class weights')
grid_rfb_score_x

predictions = pd.concat([over_rf_score_x, smote_rf_score_x, SMOTETomek_rf_score_x, grid_rfb_score_x], ignore_index=True, sort=False)
predictions.sort_values(by=['Recall'], ascending=False)

